---
title: 第一次使用Linux远程开发
date: 2025-02-12 12:00:00 +0800
categories: [笔记, 编程]
tags: [笔记, 编程, 深度学习, 远程开发, Linux, CUDA]
description: 事情要从大半夜11点收到老板的消息开始说起...
---

大半夜11点收到老板消息，有个紧急任务要给我做，项目的具体内容保密不能公开，大致情况就是深度学习那些内容。

看了一眼需求，很经典的resnet50的backbone，本地的电脑自然是带不动（我自己的电脑用的3060的卡），因此也是用上Linux服务器了。

Linux的部分我只是在本科自学Java那会的时候，拿CentOS稍微看过一点，但是当时也没怎么用，所以这次用说是从零开始也不为过了。

本则笔记大概记录一下使用过程中比较有意思的一些内容。

## Get Everything Started

如果要使用服务器的话，第一点肯定是要连接到服务器。

给的教程是CSDN的这篇[vscode连接远程服务器（傻瓜式教学）](https://blog.csdn.net/zhaxun/article/details/120568402)，用自己常用的VSCode环境做开发还是比较友好的，vim我自己用的实际上还不太熟。

在这个基础上，不算太难，相当于就是一个SSH的配置，跟自己提交Github大差不差。

## 脑溢血的CUDA环境

看到服务器上的环境说真的眼前一黑，来自facebook的经典[Detectron2框架](https://github.com/facebookresearch/detectron2)。

本科的时候当时是做UNet和Mask R-CNN，后者就是用的这个框架，跑模型性能差时间长，还难改的一比，最后Down掉了用的UNet，对这个框架有心理阴影了。现在又要用这个，心里默念这个框架是真的阴魂不散。

当然这次不是用的完完全全的就这个框架就完了，实际上用的[Mask2Former](https://github.com/facebookresearch/Mask2Former)，这个框架是打底子了。

这里Former就是Transformer，QKV那一套，本科的时候也接触过，当时是做弱监督用，然而也是时间长效果差最后也没做下去了。

老实说这次能不能搞定这个项目心里真的没底，没辙硬着头皮做吧。

Detectron2这个框架对CUDA版本的要求相当严格，深度学习搭环境的老毛病了。

服务器上面有队友按开发文档（然而开发的库在今年年初就archived了），它是这么写的：

```bash
conda create --name mask2former python=3.8 -y
conda activate mask2former
conda install pytorch==1.9.0 torchvision==0.10.0 cudatoolkit=11.1 -c pytorch -c nvidia
pip install -U opencv-python

# under your working directory
git clone git@github.com:facebookresearch/detectron2.git
cd detectron2
pip install -e .
pip install git+https://github.com/cocodataset/panopticapi.git
pip install git+https://github.com/mcordts/cityscapesScripts.git

cd ..
git clone git@github.com:facebookresearch/Mask2Former.git
cd Mask2Former
pip install -r requirements.txt
cd mask2former/modeling/pixel_decoder/ops
sh make.sh
```

我自己尝试了一下本地部署，直接卡在`sh make.sh`这里，C我根本没学过，报一大堆啥玩意我也看不懂。后来才发现环境要求有一条是`Linux or macOS with Python ≥ 3.6`，我本地是Windows，所以直接放弃了。

服务器那边是Linux倒没问题，队友跟我说eval没问题，自己跑也确实没问题。

但是到了train和inference的时候就开始各种报错，还是C的，没办法丢给deepseek了，最后定位到：

`conda install pytorch==1.9.0 torchvision==0.10.0 cudatoolkit=11.1 -c pytorch -c nvidia`

这里的cudatoolkit是11.1，nvidia-smi显示服务器上的卡是CUDA 12.0，所以肯定是不兼容的。

此外，系统的CUDA_HOME的值是空的，需要手动设一下值，也算是复习vi了。

### 设置CUDA_HOME

有读写权限但是没sudo权限，不敢乱搞。

1. `vim ~/.bashrc`定位到环境变量。
2. 添加了形如以下内容到环境变量的末尾：
```
# ADD CUDA HOME
export CUDA_HOME=/usr/local/cuda
export PATH=$PATH:$CUDA_HOME/bin
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CUDA_HOME/lib64
```
3. 保存退出（`esc`，`:wq`）
4. `source ~/.bashrc`使环境变量生效。
5. `nvcc -V`检查是否生效。

### 升级CUDA

没有卸载整个环境，用依赖管理尝试调整torch和cuda toolkit版本，最后确认cuda toolkit 11.7可用。

使用了`conda install pytorch==1.13.1 torchvision==0.14.1 cudatoolkit=11.7 -c pytorch -c nvidia`。

这个包有一说一下的是真的慢。

完成安装之后，重新走一遍流程，在`sh make.sh`前要手动`rm -rf build`清一下缓存。

## tmux

用VSCode跑训练进程的时候不可能整天守着+保持自己电脑开机，但是远端的服务器肯定是一直在跑的，这时候有个东西能监听session就好。

tmux我觉得是很符合直觉的一个工具，它就是干这个的。

使用方法参见[阮一峰的网络日志：Tmux 使用教程](https://www.ruanyifeng.com/blog/2019/10/tmux.html)。

## 单独指定一张卡来训练

~~是的这么一个简单问题居然要单独写一段。~~

官方文档的描述为：

```markdown
* To run __on cpu__, add `MODEL.DEVICE cpu` after `--opts`.
```

然而并没有什么卵用。

尝试了通配的`CUDA_VISIBLE_DEVICES=2,3 python3 xxx.py`似乎也没效果，一看还是在cpu0上面。

最后在[Stack Overflow](https://stackoverflow.com/questions/39649102/how-do-i-select-which-gpu-to-run-a-job-on)上面找到的结果。Github上也有对应的[Issue](https://github.com/facebookresearch/detectron2/issues/210)。

解决方案相当于是把窗口下的环境变量改了，如下：

```bash
export CUDA_VISIBLE_DEVICES=1
python xxx.py
```

后面阅读了`detectron2\engine\launch.py`的源码，下面有这样一段：

```python
def _distributed_worker(*args,**kargs):
'''
other codes
'''
# Setup the local process group.
    comm.create_local_process_group(num_gpus_per_machine)
    if has_gpu:
        torch.cuda.set_device(local_rank) # ←直接分配CPU了
```

默认使用分布式，且默认自己分配CPU，无语了。

推测新开进程与当前进程会不在一个进程里面，导致通配方案会直接覆盖不到训练进程，然后就又找cpu0去了。

## 谁更强？

~~没什么用的小知识增加了。~~

使用`nvidia-smi`居然会看不全GPU名字，看到了一行：

`|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  Off |`

气笑了，解决方案是`nvidia-smi --query-gpu=name --format=csv,noheader`。

目前组里用的两张卡是NVIDIA GeForce RTX 4090和NVIDIA RTX A6000。乍一听前者显存24G，后者显存48G，感觉应该是后面那个厉害点，但发现后者跑模型明显速度不对劲。

找GPT要了个规格对比图。后起之秀，更耗电，核心更多，也难怪跑的更快了。

| 参数              | NVIDIA GeForce RTX 4090        | NVIDIA RTX A6000               |
| ----------------- | ------------------------------ | ------------------------------ |
| **架构**          | Ada Lovelace (AD102)           | Ampere (GA102)                 |
| **CUDA 核心数**   | 16384                          | 10752                          |
| **Tensor 核心数** | 512                            | 336                            |
| **显存容量**      | 24 GB GDDR6X                   | 48 GB GDDR6X                   |
| **显存带宽**      | 1008 GB/s                      | 700 GB/s                       |
| **功耗**          | 450W                           | 300W                           |
| **基础频率**      | 2235 MHz                       | 1395 MHz                       |
| **加速频率**      | 2520 MHz                       | 1695 MHz                       |
| **多精度性能**    | 高（支持 FP32、FP16、INT8 等） | 高（支持 FP32、FP16、TF32 等） |
| **发布日期**      | 2022年10月                     | 2020年12月                     |

另外，实测下来`IMS_PER_BATCH=2`只占16G左右的显存，但是GPU利用率已经干到70~80，往上加BATCH_SIZE虽然显存都用上了，但是训练速度却因为GPU满负荷反而会降低，这就很乐了。